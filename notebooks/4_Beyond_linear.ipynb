{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Linear: Going Hierarchical\n",
    "All nice and well but linear models with only one predictor can be a bit boring, right? Also, most people know that size is not the only factor determining the rental price. If you've lived in Berlin for a while, you know that certain areas are much more expensive than others.\n",
    "\n",
    "Unfortunately, this data set doesn't contain the coordinates for each flat nor the exact address. But for each flat we have a ZIP code (the PLZ).\n",
    "We will now extend our model to incorporate the location by using the ZIP code. We do so by training one linear model per ZIP code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import standardize_area, destandardize_area\n",
    "\n",
    "berlin = pd.read_csv(\"../data/berlin.csv\", index_col=0, dtype={\"geo_plz\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.rcParams['figure.figsize'] = 9, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One model per zip code works well if a zip code has many observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=berlin[\"livingSpace\"][berlin.geo_plz == \"13583\"], y=berlin[\"totalRent\"][berlin.geo_plz == \"13583\"], \n",
    "            color=\"#42b883\", ci=0, scatter_kws={\"s\": 40}, label=\"Spandau\")\n",
    "sns.regplot(x=berlin[\"livingSpace\"][berlin.geo_plz == \"10405\"], y=berlin[\"totalRent\"][berlin.geo_plz == \"10405\"], \n",
    "            color=\"#f07855\",  ci=0, scatter_kws={\"s\": 40}, label=\"Prenzlauer Berg\")\n",
    "\n",
    "plt.scatter(berlin[\"livingSpace\"], berlin[\"totalRent\"], s=3, c=\"gray\", alpha=0.3)\n",
    "plt.ylabel(\"Monthly Rent [â‚¬]\")\n",
    "plt.xlabel(\"Living Area [sqm]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a zip code has only a handful or less observations, it is a bit more difficult. In this zip code, we only have two observations and a model fit on these two points would result in a negative slope which doesn't make much sense. In cases where we have little data, we would prefer the model to be closer to a model fit on all data, as we did before. Since even in Blankenburg, in general, bigger flats should be more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(berlin[\"livingSpace\"], berlin[\"totalRent\"], s=3, c=\"gray\", alpha=0.3)\n",
    "plt.scatter(berlin[\"livingSpace\"][berlin.geo_plz == \"13129\"], \n",
    "            berlin[\"totalRent\"][berlin.geo_plz == \"13129\"], s=40, label=\"Blankenburg\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve this by saying that for each linear model (one for each zip code), the parameters determining the slope and intercept of this model come from a common distribution. So all slope parameters for example would come from a Normal distribution centered around some value close to 4.4 (the slope parameter we obtained in our last model) but the slope for Prenzlauer Berg would be higher than average while Spandau would be lower than average and places like Blankenburg would stay close to the mean (and also have higher uncertainty).\n",
    "\n",
    "Before, our model looked like this:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{rent} &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n",
    "\\mu &= \\alpha + \\beta \\text{area} \\\\\n",
    "\\\\\n",
    "\\alpha &\\sim \\text{Normal}(0, 10) \\\\\n",
    "\\beta &\\sim \\text{Normal}(0, 5) \\\\\n",
    "\\\\\n",
    "\\sigma &\\sim \\text{HalfNormal}(5) \n",
    "\\end{align*}$$\n",
    "\n",
    "We know extend this as follows:\n",
    "$$\\begin{align*}\n",
    "\\text{rent} &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n",
    "\\mu &= \\alpha_{[\\text{ZIP}]} + \\beta_{[\\text{ZIP}]} \\text{area} \\\\\n",
    "\\\\\n",
    "\\alpha_{[\\text{ZIP}]} &\\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha}) \\\\\n",
    "\\beta_{[\\text{ZIP}]} &\\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta}) \\\\\n",
    "\\\\\n",
    "\\mu_{\\alpha} &\\sim \\text{Normal}(0, 10) \\\\\n",
    "\\mu_{\\beta} &\\sim \\text{Normal}(0, 5) \\\\\n",
    "\\\\\n",
    "\\sigma, \\sigma_{\\alpha}, \\sigma_{\\beta} &\\sim \\text{HalfNormal}(5) \n",
    "\\end{align*}$$\n",
    "\n",
    "This looks like an awful lot more formula but the most important changes are only in the second, third and fourth line, the lines below are mostly repeating priors from above.\n",
    "\n",
    "In the second line, we use a linear model similar to above, but with different $\\alpha$ and $\\beta$ for each ZIP code. As before, we need to define priors for these two parameters. Only unlike above, we now put so called hyperpriors on the parameters of these priors. $\\mu_{\\alpha}$ is now the expected mean for the intercepts of each ZIP code and $\\sigma_{\\alpha}$ determines how much of a difference there can be between the intercept in Spandau and the intercept in Prenzlauer Berg.\n",
    "\n",
    "Don't worry if so many formulas are not your cup of tea, we now gonna look at the code for the model. Before writing the model however, we need to map the zip codes to an index variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"zip\"] = berlin.geo_plz.map(str.strip)\n",
    "zip_codes = np.sort(berlin.zip.unique())\n",
    "num_zip_codes = len(zip_codes)\n",
    "zip_lookup = dict(zip(zip_codes, range(num_zip_codes)))\n",
    "berlin[\"zip_code\"] = berlin.zip.replace(zip_lookup).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a small helper function to map from ZIP string to ZIP code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_zip_codes(zip_strings, zip_lookup=zip_lookup):\n",
    "    return pd.Series(zip_strings).replace(zip_lookup).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hier_model:\n",
    "    mu_alpha = ...\n",
    "    sigma_alpha = ...\n",
    "    \n",
    "    mu_beta = ...\n",
    "    sigma_beta = ...\n",
    "    \n",
    "    alpha = ...\n",
    "    \n",
    "    beta = ...\n",
    "    \n",
    "\n",
    "    sigma = ...\n",
    "    \n",
    "    mu = ...\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sd=sigma, observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    trace = pm.sample(random_seed=2020, chains=4, \n",
    "                      draws=1000, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to do the convergency checks again. For this, we first collect the model artifacts in an ArviZ Data object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "pm_data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing where the ArviZ Data object is very useful, is for dealing with high-dimensional model artificats. With hierararchical models such as this one, we get one parameter per level (here the ZIP codes) and 4000 samples per parameter, coming from four chains (at least in the beginning, we want to keep the samples of each chain separated). ArviZ makes it easier to keep track of all the different dimensions and coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the dimensions `alpha_dim_0` and `beta_dim_0` that represent the ZIP codes. We have 208 ZIP codes and since we mapped them to an index, the coordinates for these dimensions are simply the integers from 0 to 207. But we can give them meaningful names by providing ArviZ with the original ZIP code strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = az.from_pymc3(model=hier_model, trace=trace,\n",
    "                        # create a new coordinate\n",
    "                        ...\n",
    "                        # this coordinate is used by the dimension alpha and beta\n",
    "                       ...\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's much more that can be done with the ArviZ data object but this would go beyond this tutorial. A small introduction on how to work with them can be found [here](https://github.com/corriebar/arviz/blob/xarray-example/doc/notebooks/Working%20with%20InferenceData.ipynb).\n",
    "\n",
    "\n",
    "Next, we'll check the trace plots. Remember, ArviZ has a function for this. Only problem: we know have many many alpha and beta parameters, one each for each ZIP code. This is way too much to plot! Use the function parameter `var_names` to only select the parameters from the model that don't use ZIP code as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check the three different summary/diagnostic statistics. There were R_hat, MCSE (Monte Carlo Standard Error), and ESS (Effective Sample Size). You can get the summary table using ArviZ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, since we have so many parameters, we can't check them easily by hand. What we can do instead, is to plot a histogram for each of the diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rhat\n",
    "summ = az.summary(pm_data, round_to=5)\n",
    "\n",
    "...\n",
    "plt.title(\"R_hat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for yourself the ESS diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ess\n",
    "...\n",
    "plt.title(\"ESS Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we checked how good our model fit the data by comparing the plot of the linear model to our data. Since we now have a collection of linear models, this would be rather difficult. What we can do instead is a so called posterior-predictive check. We compare the predicted distribution of outcomes to the actual distribution of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hier_model:\n",
    "    posterior_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(14,14), sharex=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "...\n",
    "\n",
    "ax[0].set_title(\"Original data\")\n",
    "\n",
    "sample_nums = np.random.choice(posterior_pred[\"rent\"].shape[0], size=3, replace=False)\n",
    "for i, samp in enumerate(sample_nums):\n",
    "    \n",
    "    ...\n",
    "    ax[i+1].set_title(f\"Sample {i+1}\")\n",
    "plt.suptitle(\"Comparing Original Data to Predicted Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is difficult to visualize all models, we can pick out a few and check how the model differs for different ZIP codes. To now be able to select the posterior sample of a single ZIP code, the ArviZ object becomes very helpful. If we, for example, want to extract the sample for Blankenfelde (the ZIP code 13129 from above with so few observations), we get the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blankenfelde = ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_s = np.linspace(start=-2, stop=3.5, num=50)\n",
    "\n",
    "mu_pred_blankenfelde = ...\n",
    "\n",
    "\n",
    "# destandardize area again\n",
    "area = destandardize_area(area_s)\n",
    "\n",
    "plt.plot(area, mu_pred_blankenfelde.mean(1)*100, alpha=0.3, c=\"k\")\n",
    "plt.scatter(berlin[\"livingSpace\"], berlin[\"totalRent_s\"]*100, s=4, alpha=0.4, c=\"grey\")\n",
    "\n",
    "plt.title(\"Uncertainty (mu) for Blankenburg\")\n",
    "\n",
    "\n",
    "az.plot_hpd(area, mu_pred_blankenfelde.T*100, credible_interval=0.83)\n",
    "\n",
    "plt.scatter(berlin[\"livingSpace\"][berlin.geo_plz == \"13129\"], \n",
    "            berlin[\"totalRent\"][berlin.geo_plz == \"13129\"], s=40, label=\"Blankenburg\")\n",
    "plt.xlabel('Living Area [sqm]')\n",
    "plt.ylabel('Rent [â‚¬]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with one of the ZIP codes from above that had more data.\n",
    "Go and make the same plot for a different ZIP code (of your choice)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can also check how the full uncertainty looks like for these ZIP codes. Remember, for this you'll need to compute the predictions for rent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_blankenfelde = ...\n",
    "\n",
    "plt.plot(area, mu_pred_blankenfelde.mean(1)*100, alpha=0.3, c=\"k\")\n",
    "plt.scatter(berlin[\"livingSpace\"], berlin[\"totalRent_s\"]*100, s=4, alpha=0.7, c=\"grey\")\n",
    "\n",
    "az.plot_hpd(area, mu_pred_blankenfelde.T*100, credible_interval=0.83, \n",
    "            fill_kwargs={\"alpha\": 0.5})\n",
    "\n",
    "az.plot_hpd(area, rent_blankenfelde.T*100, credible_interval=0.83, \n",
    "            fill_kwargs={\"alpha\": 0.5})\n",
    "plt.scatter(berlin[\"livingSpace\"][berlin.geo_plz == \"13129\"], \n",
    "            berlin[\"totalRent\"][berlin.geo_plz == \"13129\"], s=40, label=\"Blankenburg\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Full uncertainty for Blankenfelde\")\n",
    "plt.xlabel('Living Area [sqm]')\n",
    "plt.ylabel('Rent [â‚¬]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for ZIP code of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing the rent predictions by hand, we could also use PyMC data container to handle predictions on new data.\n",
    "\n",
    "Unfortunately, because of an still unclosed issue, we can't use the PyMC data container to update the ZIP code indices but need to use a shared theano variable. However, both types are updated in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = ...\n",
    "\n",
    "with pm.Model() as hier_model:\n",
    "    \n",
    "    area = ...\n",
    "    \n",
    "    mu_alpha = pm.Normal(\"mu_alpha\", mu=0, sigma=10)\n",
    "    sigma_alpha = pm.HalfNormal(\"sigma_alpha\", sigma=5)\n",
    "    \n",
    "    mu_beta = pm.Normal(\"mu_beta\", mu=0, sigma=5)\n",
    "    sigma_beta = pm.HalfNormal(\"sigma_beta\", sigma=5)\n",
    "    \n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_alpha, sd=sigma_alpha, \n",
    "                      shape=num_zip_codes)\n",
    "    \n",
    "    beta = pm.Normal(\"beta\", mu=mu_beta, sd=sigma_beta, \n",
    "                     shape=num_zip_codes)\n",
    "    \n",
    "\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n",
    "    \n",
    "    mu = alpha[zips] + beta[zips]*area\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sd=sigma, observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    trace = pm.sample(random_seed=2020, chains=4, \n",
    "                      draws=1000, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the area and ZIP code data to for example your own flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_flats = pd.DataFrame({\"area\": standardize_area(np.array([100, 240, 74])), \n",
    "                           \"zip_code\": [\"10243\", \"10179\", \"12047\"]})\n",
    "\n",
    "more_flats[\"zip\"] = map_zip_codes(more_flats[\"zip_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hier_model:\n",
    "    zips.set_value(more_flats[\"zip\"])\n",
    "    pm.set_data({\"area\": more_flats[\"area\"]})\n",
    "    post_pred = pm.sample_posterior_predictive(trace, samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can now plot this as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = post_pred[\"rent\"][:,2]*100\n",
    "\n",
    "plt.hist(y_pred, ec=\"darkblue\", alpha=0.9, bins=20)\n",
    "plt.title(\"Rental price distribution\\nfor a flat of 74sqm in 12047\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or ask for the probability that your flat would have a rent lower than your own rent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyLadies-Bayesian-Tutorial",
   "language": "python",
   "name": "pyladies-bayesian-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
