{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting simple: Linear Regression\n",
    "As so often, we will start relatively simple with a Linear Regression. Sounds maybe a bit boring but don't worry, I will show you later how to extend this model to something slightly more complex.\n",
    "\n",
    "## Short Look at the Data\n",
    "For this tutorial, I will use rental data that I scraped from Immoscout24. For a more detailed description and information on how I scraped the data set, you can check its description on [kaggle](https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany) where I occasionally also update the data.\n",
    "For this analysis, we will concentrate on rental offers in Berlin but of course feel free to try out different cities or areas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from utils import iqr, iqr_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")   # or your own favorite style\n",
    "plt.rcParams['figure.figsize'] = 9, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"../data/immo_data.csv\", dtype={\"geo_plz\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data, we will do a bit of preprocessing: We remove outliers where either the living area or the total rent is too low or too high. To remove outliers, the [Inter quartile range (IQR)](https://en.wikipedia.org/wiki/Interquartile_range#Outliers) is used. The IQR rule marks everything as outlier that's too far from the middle range of the data. Most of the data we throw away this way were typos or similar with unreasonable input values.\n",
    "For a more thorough analysis, it might be useful to check that we don't throw away real cases and instead incorporate the outliers in further analysis.\n",
    "\n",
    "As target variable we will use the variable `totalRent`, which in most cases is the sum of the base rent and a service fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"totalRent\"] = np.where(d[\"totalRent\"].isnull(), d[\"baseRent\"], d[\"totalRent\"])\n",
    "\n",
    "# since log doesn't work with 0, we replace 0 with 0.5\n",
    "# seems reasonable tto say hat a rent of 0€ is the same as 50ct\n",
    "d[\"livingSpace_m\"] =  np.where(d[\"livingSpace\"] <= 0, 0.5, d[\"livingSpace\"])\n",
    "d[\"totalRent_m\"] = np.where(d[\"totalRent\"] <= 0, 0.5, d[\"totalRent\"])\n",
    "d[\"logRent\"] = np.log(d[\"totalRent_m\"])\n",
    "d[\"logSpace\"] = np.log(d[\"livingSpace_m\"])\n",
    "\n",
    "not_outlier = iqr_rule(d[\"logSpace\"], factor=1.5) & iqr_rule(d[\"logRent\"], factor=1.5)\n",
    "d = d[not_outlier]\n",
    "berlin = d[(d.regio1 == \"Berlin\") ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we want to predict the rent (`totalRent`) by the living area (`livingSpace`).\n",
    "\n",
    "You can have a short look at the data and these two variables!\n",
    "\n",
    "For example:\n",
    "- What is the average rent in Berlin?\n",
    "- What is the average size of a flat in Berlin?\n",
    "- Plot rent vs living space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rent\n",
    "berlin[\"totalRent\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"totalRent\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in mean and median suggests that the distribution is not normally distributed. If you plot the histogram, you can also see that the distribution is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average living area\n",
    "berlin[\"livingSpace\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"livingSpace\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin.date.value_counts().plot.bar()\n",
    "plt.title(\"Number of cases per scrape time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[[\"totalRent\", \"livingSpace\", \"noParkSpaces\", \"picturecount\", \"yearConstructed\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Living space is strongly positively correlated with the total rents, as we would expect. Interestingly, the construction year doesn't seem to be strongly correlated with total rent, this might be because it is not a linear relationship: Altbau tends to be very expensive in Berlin, Plattenbau from the 70s less so and flats in new buildings are expensive again. Picture count is also positively correlated with the total rent but is a good example that correlation does not imply causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"livingSpace\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"totalRent\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"totalRent\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(berlin.livingSpace, berlin.totalRent, s=4)\n",
    "plt.title(\"Rent by living area\")\n",
    "plt.xlabel(\"Living Area [sqm]\")\n",
    "plt.ylabel(\"Monthly Rent [€]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with the data, we will rescale and normalize the living area and also rescale the total rent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin[\"livingSpace_s\"] = (berlin[\"livingSpace\"] - berlin[\"livingSpace\"].mean()) / np.std(berlin[\"livingSpace\"])\n",
    "berlin[\"totalRent_s\"] = berlin[\"totalRent\"] / 100\n",
    "\n",
    "# saving for later\n",
    "berlin.to_csv(\"../data/berlin.csv\", encoding=\"utf-8\", quoting=csv.QUOTE_NONNUMERIC) # special quoting necessary because otherwise description messes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to standardize/destandardize area a few times, so we will use small helper functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_area(x):\n",
    "    return ( x - berlin[\"livingSpace\"].mean()) / np.std(berlin[\"livingSpace\"])\n",
    "    \n",
    "def destandardize_area(x):\n",
    "    return ( x * np.std(berlin[\"livingSpace\"]) ) + berlin[\"livingSpace\"].mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(berlin.livingSpace_s, berlin.totalRent_s, s=4)\n",
    "plt.title(\"Rent by living area\")\n",
    "plt.xlabel(\"Living Area [sqm]\")\n",
    "plt.ylabel(\"Monthly Rent [100€]\")\n",
    "plt.axvline(x=0, c=\"k\", linewidth=1)\n",
    "plt.axhline(y=np.mean(berlin.totalRent_s), c=\"k\", linewidth=1, linestyle=\"--\", label=\"Average Rent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, it roughly looks like a linear relationship between area and monthly rent. Indeed, the bigger the flat, the more expensive it should be. So we will start our modelling with a linear model.\n",
    "\n",
    "So in math notation, our model can be written as follows:\n",
    "\n",
    "$$ \\text{rent} \\approx \\alpha + \\beta \\text{area} $$\n",
    "\n",
    "This is the same as\n",
    "$$ \\text{rent} = \\alpha + \\beta \\text{area} + \\epsilon$$\n",
    "where $\\epsilon$ is normally distributed, i.e. $\\epsilon \\sim \\text{Normal}(0, \\sigma)$. This can be rewritten as\n",
    "$$ \\text{rent} \\sim \\text{Normal}(\\alpha + \\beta \\text{area}, \\sigma).$$\n",
    "\n",
    "For easier reading, we rewrite this again:\n",
    "$$\\begin{align*} \\text{rent} &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n",
    "\\mu &= \\alpha + \\beta \\text{area}\n",
    "\\end{align*}$$\n",
    "This will be our first model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Bayesian ingredient: Priors\n",
    "\n",
    "Before implementing this model, let's shortly think about what the parameters $\\alpha$ and $\\beta$ mean here.\n",
    "\n",
    "We will use the model with our rescaled data and thus $\\alpha$ the intercept is the rental price of an average sized flat. (For averaged sized flats, the scaled area is 0).\n",
    "\n",
    "$\\beta$ is then the increase in rent if the flat is one standard deviation larger. One standard deviation is 29sqm which is roughly the average size for a room (check living space divided by number of rooms _noRooms_). Thus $\\beta$ is roughly the increase in rent if the flat would have one more room. \n",
    "\n",
    "$\\sigma$ is how much the rent can differ for two flats of the same size. Concretely, it is how much the rent can differ from the average rent for flats of the same size. As our model says that the rent is normally distributed, about 95% of the cases should be within $2\\sigma$ of the average rent.\n",
    "As error term, $\\sigma$ is always positive.\n",
    "\n",
    "Thinking about what the parameters mean beforehand is very important in a Bayesian analysis since we need to specify priors. Priors are what we think the parameters could be before seeing the data. And, obviously, to be able to say what range the parameters would be in, it would be good to know what the parameters mean.\n",
    "\n",
    "\n",
    "If we don't know anything about the problems, we might want to specify priors that are very uninformative and vague. \n",
    "We could for example specify $\\alpha$ and $\\beta$ as being somewhere between -10,000 and + 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod:\n",
    "    alpha = pm.Normal('alpha', mu=0, sigma=10000)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyMC-Model is specified in a context. Before we can actually specify the model, we need to specify the priors, since, as usual in Python, each variable we want to use need to declared beforehand.\n",
    "In PyMC, you always need to specify the name of the variable twice. This is so that the variable knows its own name.\n",
    "\n",
    "If you print the model, it renders nicely in a more mathy looking description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add to the model by opening the context again.\n",
    "To for example add a prior for sigma, we can proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mod:\n",
    "    sigma = pm.HalfNormal('sigma', sigma=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\sigma$ as error term is always a positive variable, we need to use a distribution that is also always positive. One commonly used distribution for this is the Half-Normal. A normal distribution that is cut in half and only positive.\n",
    "Other commonly used distributions for $\\sigma$ are for example the Exponential or the Half-Cauchy.\n",
    "\n",
    "Now that we specified some priors, we can write out the complete model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mod:\n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal('rent',mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyMC-Model is written very similar to how the model was specified above. \n",
    "\n",
    "Note that for the outcome variable, we need to specify the observed data.\n",
    "Usually the whole model is written as one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10000)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=10000)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=10000)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"] \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially in the beginning, when starting out with Bayesian modelling, picking priors can seem a bit daring. \n",
    "Does it make a difference if we use $\\text{Normal}(0,100)$ or $\\text{Normal}(0, 1000)$?\n",
    "What's with using different distributions?\n",
    "\n",
    "\n",
    "There are a few tips that help a bit with picking a good prior. The first one is to visualize your priors. We can do this with PyMC by sampling from our priors.\n",
    "This is the similar to sampling from the specified distributions using numpy or scipy. However, on top of sampling from the probability distributions, it then also computes $\\mu$ (the linear part of the model) using the samples and the predictor variables. It then uses the computed $\\mu$ and the samples from $\\sigma$ to sample possible outcome values. Even though we specified the target variable (in Machine Learning this one is usually called `y`) it does not use this (yet) and is thus very quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mod:\n",
    "    priors = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [ArviZ](https://arviz-devs.github.io/arviz/) to keep track of the different artifacts computed from our model and to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "pm_data = az.from_pymc3(prior = priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArviZ comes with many plots that are useful to analyze Bayesian models.\n",
    "Let's start looking at the priors for our three model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_density(pm_data, \n",
    "                # we want to plot the prior\n",
    "                group=\"prior\", \n",
    "                # and only the following variables\n",
    "                var_names=[\"alpha\", \"beta\", \"sigma\"],\n",
    "                # just some settings to make it prettier\n",
    "               shade=0.3, bw=8, figsize=(20,6), credible_interval=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look super interesting, it is basically just a density plot of the distributions we gave for the priors.\n",
    "More interesting is to visualize what rental prices these prior distributions would produce given the predictor data.\n",
    "Unfortunately, there is no ArviZ plot for this yet, but we can do this ourselves without too much work.\n",
    "\n",
    "The object `priors` contains a numpy array for `rent`. Check what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors[\"rent\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each observation in our dataframe (7092 obs) it computed 1000 samples for possible rent prices, using the samples from `alpha`, `beta`, and `sigma` together with the corresponding living area from this observation.\n",
    "\n",
    "We can flatten the matrix to obtain one big array and plot a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(priors[\"rent\"].flatten()*100, alpha=0.9, ec=\"darkblue\", bins=70)\n",
    "plt.title(\"Histogram over possible range of rental prices\")\n",
    "plt.xlabel(\"Monthly Rent [€]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the histogram over the actual rental prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(berlin[\"totalRent_s\"]*100, alpha=0.9, ec=\"darkblue\", bins=70)\n",
    "plt.title(\"Histogram over the actual rental prices\")\n",
    "plt.xlabel(\"Monthly Rent [€]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms don't look very similar and the range of the sampled rents is completly off! A rent of up to 50,000,000€ per month doesn't sound very realistic.\n",
    "\n",
    "\n",
    "Another good way to understand the prior better is to visualize the model that it would produce. In our case, the model is a line determined by the intercept $\\alpha$ and the slope $\\beta$. \n",
    "We can thus sample 50 $\\alpha$ and $\\beta$s and multiply this with the range of area values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_s = np.linspace(start=-2, stop=3.5, num=50)\n",
    "draws = np.random.choice(len(priors[\"alpha\"]), replace=False, size=50)\n",
    "alpha = priors[\"alpha\"][draws]\n",
    "beta = priors[\"beta\"][draws]\n",
    "\n",
    "mu = alpha + beta * area_s[:, None]\n",
    "\n",
    "\n",
    "plt.plot(destandardize_area(area_s), mu*100, c=\"#737373\", alpha=0.5)\n",
    "plt.xlabel(\"Living Area [sqm]\")\n",
    "plt.ylabel(\"Price [€]\")\n",
    "plt.title(\"Linear model according to our prior\")\n",
    "plt.axhline(y=900, color=\"#fc4f30\", label=\"My own rent\")\n",
    "plt.axhline(y=5000, color='#e5ae38', label=\"Most expensive realistic rent I can think of\")\n",
    "plt.axhline(y=0, label=\"Free rent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Use `plt.axhline(y=my_rent)` to compare the sampled lines with your own rent, the most expensive, reasonable rent you can think off and the cheapest possible rent you can think off. I also like to google what is the known most expensive rent one can find in Berlin (or elsewhere) and see how it compares to the model.\n",
    "\n",
    "You could also add these benchmarks to the histogram using `plt.axvline(x=my_rent)`.\n",
    "\n",
    "These priors are obviously not very realistic! Luckily, we all know a bit about rents, so even without looking at the data, we can think of better priors.\n",
    "\n",
    "Write a new model `mod_informed` that uses the same linear part as above, but better priors. You can use the same distributions as above and only change its parameters. I recommend you to try various priors and check what effects they have on the resulting model and its outputs.\n",
    "\n",
    "Remember: \n",
    "- $\\alpha$ the intercept is the rental price of an average sized flat (averaged sized flat is 77sqm).\n",
    "- $\\beta$ the slope is roughly the increase in rent if the flat would have one more room\n",
    "- $\\sigma$ is how much the rent can differ for two flats of the same size. As error term, $\\sigma$ is always positive.\n",
    "\n",
    "Always keep in mind the scale of your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_informed:\n",
    "    # these are definitely not the only good priors for this problem\n",
    "    # for this problem, many priors, even the original ones,\n",
    "    # work quite fine for this problem\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=5)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    priors = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made functions for the above plot, so you don't have to copy the whole plot code: `compare_hist(priors, berlin)` to plot the two histograms and `draw_models(priors, berlin)` to plot sampled lines from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compare_hist, draw_models\n",
    "\n",
    "compare_hist(priors, berlin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_models(priors, berlin)\n",
    "plt.axhline(y=900, color=\"#fc4f30\", label=\"My own rent\")\n",
    "plt.axhline(y=5000, color='#e5ae38', label=\"Most expensive realistic rent I can think of\")\n",
    "plt.axhline(y=0, label=\"Free rent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The priors still cover a very wide range of values, even some values that are not very realistic. It still covers negative values, has negative slopes, and goes up to some still relatively extreme values. But that's fine, afterall, the model will still see the data and then learn from that. But this way we managed to tell the model a plausible range of possible values.\n",
    "\n",
    "\n",
    "Note: It is not good practise to deduce priors from the data we will train on later. So comparing the prior distributions to the original distribution is not something you should do to get the prior distribution close to the data distribution. I only presented the data here to help get an idea what ranges we're talking about. It's usually better to deduce priors from domain knowledge or external knowledge and not from the data itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to explore regarding different priors:\n",
    "- We've seen above what happens when we use overly vague priors, what happens if we use very narrow priors? What problems could arise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_narrow:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=0.5)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    priors_narrow = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_hist(priors_narrow, berlin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the model covers a very small range of values it thinks are plausible and unfortunately, the real range is not included in this range!\n",
    "Using very (too) narrow priors is the same as using (too) heavy regularization in Machine Learning: The model underfits and cannot learn from the data because we restrict it too strongly. \n",
    "\n",
    "In this case, since the normal distribution puts at least a tiny tiny bit of probability mass even on far away values and we have a lot of data, the model still manages to learn the correct values. But we're making it quite difficult for the model! If we'd have less data this would certainly not be the case.\n",
    "\n",
    "A good source on which priors to pick for which problem is the [Stan Prior Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we've only used the normal distribution for the priors. Try out some different distributions, e.g.\n",
    "    - Uniform distribution, e.g. over -100 to + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_uniform:\n",
    "    alpha = pm.Uniform(\"alpha\", lower=-100, upper=100)\n",
    "    beta = pm.Uniform(\"beta\", lower=-100, upper=100)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    priors_uniform = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_hist(priors_uniform, berlin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive distributions looks actually quite similar to the ones we got above. In this case, the results will also look very similar.\n",
    "\n",
    "\n",
    "However, when we use a uniform prior distribution, we introduce hard borders. Hard borders are generally not a good idea: \n",
    "They can make it difficult for the sampling algorithm (using for example $\\text{Uniform}(-1000, +1000)$ beaks the sampling!) but they're especially problematic when the true parameter value is not inside this hard border range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_uniform_narrow:\n",
    "    alpha = pm.Uniform(\"alpha\", lower=-0.5, upper=0.5)\n",
    "    beta = pm.Uniform(\"beta\", lower=-0.5, upper=0.5)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    trace_uniform_narrow = pm.sample(draws=1000, tune=1000)\n",
    "    priors_uniform_narrow = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model puts zero probability mass on the correct parameter values: It will thus fail and always produce wrong results, no matter how much data we feed. This is not so much visible in the prior predictive distribution (except for it being far off from plausbile values) but shows in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_uniform_narrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both alpha and beta are exactly at the border of their prior distribution and if we compare it later to our actual model, we will see that these values are completely off.\n",
    "\n",
    "So unless you have good reasons for it, I would not recommended to use the Uniform distribution as prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Student-T distribution\n",
    "    \n",
    "The Student-T distribution is often used instead of the Normal distribution for the rent distribution. The t-distribution has much heavier tails which makes it much more robust against outlier. it is thus often also called a robust regression. To use the T-distribution, we will have to estimate another parameter $\\nu$ nu, the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_robust:\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=5)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    # nu, the degrees of freedom is a new parameter\n",
    "    # this is a commonly used default prior used for nu\n",
    "    nu = pm.Gamma(\"nu\", alpha=2, beta=0.1)\n",
    "    \n",
    "    rent = pm.StudentT(\"rent\", nu=nu, mu=mu, sigma=sigma,\n",
    "                       observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    trace_robust = pm.sample(draws=1000, tune=1000)\n",
    "    priors_robust = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_density(az.from_pymc3(prior=priors_robust, model=mod_robust), \n",
    "                group=\"prior\",\n",
    "                var_names=[\"alpha\", \"beta\", \"sigma\", \"nu\"],\n",
    "               shade=0.3, credible_interval=1, bw=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for large values of nu, the T-distribution also approaches a Normal distribution, we can even check how \"normal\" our data is. In a robust regression, it is even possible to [detect outliers](https://docs.pymc.io/notebooks/GLM-robust-with-outlier-detection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\nu$ is relatively high, indicating that our data is pretty normal. Well, kind of by design since I cleaned out the outliers before.\n",
    "You can go and try how this changes if you leave the outliers (or at least some outliers) in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we know that rents should increase for larger flats, we know that the slope $\\beta$ should be positive, how could we bias our priors to positive values?\n",
    "\n",
    "One possibility is to just shift the mean of beta a bit up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mod_pos_slope:\n",
    "    alpha = pm.Normal(\"alpha\", mu=5, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=2, sigma=5)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n",
    "    \n",
    "    mu = alpha + beta*berlin[\"livingSpace_s\"]\n",
    "    \n",
    "    rent = pm.Normal(\"rent\", mu=mu, sigma=sigma,\n",
    "                    observed=berlin[\"totalRent_s\"])\n",
    "    \n",
    "    priors_pos_slope = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_models(priors_pos_slope, berlin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, there are still a few negative slope lines but most have now a positive slope.\n",
    "\n",
    "Since there is quite a lot of data, in this problem it doesn't make too much of a difference though to use this slightly adapted prior.\n",
    "\n",
    "\n",
    "Another option is to consider a transformation of the data: For both the living area and the rental price, it might make sense to log-transform these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional exercises for at home: \n",
    "To understand better how something works, it often helps to implement it oneself. Try implementing yourself sampling from the prior using only numpy or scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing prior sampling\n",
    "n = 1000\n",
    "alpha = np.random.normal(loc=0, scale=10, size=n)\n",
    "beta = np.random.normal(loc=0, scale=5, size=n)\n",
    "\n",
    "sigma = np.random.normal(loc=0, scale=5, size=n)\n",
    "sigma = np.absolute(sigma)\n",
    "\n",
    "mu = alpha + beta*berlin[\"livingSpace_s\"][:,None]\n",
    "\n",
    "rent = np.random.normal(loc=mu, scale=sigma).reshape(1,n,len(berlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_data = az.from_dict(prior = {\"alpha\":alpha,\n",
    "                                 \"beta\":beta,\n",
    "                                 \"sigma\":sigma},\n",
    "            prior_predictive={\"rent\":rent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_density(own_data, group=\"prior\",\n",
    "               shade=0.3, bw=8, credible_interval=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {\"alpha\":alpha,\n",
    "         \"beta\":beta,\n",
    "         \"sigma\":sigma,\n",
    "         \"rent\": rent}\n",
    "compare_hist(prior, berlin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_models(prior, berlin)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyLadies-Bayesian-Tutorial",
   "language": "python",
   "name": "pyladies-bayesian-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
